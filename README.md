# Obfuscation-Attack

Adversarial machine learning is a series of techniques aimed at compromising the correct functioning of a computer system that makes use of machine learning algorithms, through the construction of special inputs capable of deceiving such algorithms: more specifically, the purpose of these techniques is to cause misclassification in one of these algorithms. 


The goal of an obfuscation (aka evasion) attack is to violate the integrity of a machine learning model.
During an obfuscation attack, the attacker modifies a certain sample with the aim of obtaining as output from the classifier a class that is different from its real class of belonging; alternatively, the attack could more simply attempt to decrease the confidence of the model for that sample. 
